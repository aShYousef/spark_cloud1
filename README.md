Spark Data Processor

A cloud-based distributed data processing and machine learning platform built using Apache Spark / PySpark, FastAPI, and React.
The system enables users to upload datasets, run parallel and distributed analytics, execute multiple machine learning jobs, and evaluate scalability and performance on cloud infrastructure.


---

ğŸ“Œ Project Requirements Coverage

This project satisfies all required specifications:

Cloud-based data processing service

Distributed and parallel processing using Apache Spark / PySpark

Dataset upload and cloud storage

Descriptive statistics (multiple metrics)

Multiple machine learning jobs using Spark MLlib

Execution on 1, 2, 4, and 8 workers

Performance measurement (execution time, speedup, efficiency)

Results visualization and download

User-friendly web interface



---

ğŸ—ï¸ Architecture Overview

The system follows a three-layer cloud architecture:

1. Frontend (React + Vite)

Dataset upload

Task selection

Job monitoring

Visualization of performance metrics



2. Backend API (FastAPI)

Handles file uploads

Stores datasets in cloud/local storage

Submits Spark jobs

Tracks job status and execution metrics



3. Distributed Processing Layer

Apache Spark / PySpark

Executed on Databricks clusters

Scales across multiple workers





---

âœ¨ Features

ğŸ”¹ Data Upload

Supports CSV, JSON, and Parquet files

Files stored in cloud storage (AWS S3 or local storage for development)


ğŸ”¹ Descriptive Statistics

At least four statistics are computed and stored, including:

Number of rows

Number of columns

Data types

Null / missing value percentages

Min / Max / Mean / Std (numeric columns)

Unique value counts


ğŸ”¹ Machine Learning Jobs (Spark MLlib)

The following five ML jobs are implemented:

1. Descriptive Statistics


2. Linear Regression


3. Logistic Regression


4. K-Means Clustering


5. FP-Growth (Frequent Pattern Mining)



Each job:

Runs in a distributed Spark environment

Outputs results to cloud storage

Displays results in the UI



---

ğŸ“ˆ Scalability and Performance Evaluation

To evaluate scalability, each ML job is executed using different cluster sizes:

1 worker

2 workers

4 workers

8 workers


For each configuration:

Execution time is recorded

Speedup and efficiency are computed


Metrics:

Speedup    = T1 / Tp
Efficiency = Speedup / p

Where:

T1 = execution time with 1 worker

Tp = execution time with p workers


Performance results are:

Displayed in the web interface

Stored in cloud storage for reporting



---

ğŸ“‚ Project Structure

spark_cloud1/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py          # FastAPI application
â”‚   â”‚   â”œâ”€â”€ models.py        # Database models
â”‚   â”‚   â”œâ”€â”€ schemas.py       # API schemas
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ spark_jobs.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ spark_ml_jobs.py     # PySpark ML jobs (Databricks)
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf
â”‚       â””â”€â”€ databricks.tf
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ databricks_deploy.py
â”‚   â”œâ”€â”€ download_sample_data.py
â”‚   â””â”€â”€ run_local.sh
â”‚
â””â”€â”€ README.md


---

ğŸš€ Quick Start (Local Development)

1ï¸âƒ£ Install Dependencies

cd backend
pip install -r requirements.txt

cd ../frontend
npm install


---

2ï¸âƒ£ Run the Application

bash scripts/run_local.sh


---

3ï¸âƒ£ Access the Application

Frontend UI:
ğŸ‘‰ http://localhost:5000

Backend API:
ğŸ‘‰ http://localhost:8000

API Documentation:
ğŸ‘‰ http://localhost:8000/docs



---

â˜ï¸ Cloud Deployment

AWS Setup

S3 bucket for dataset and result storage

IAM roles for Spark access


Databricks Setup

Spark clusters configured with 1â€“8 workers

PySpark notebook uploaded automatically

Jobs executed remotely via Databricks REST API



---

ğŸ”Œ API Reference

Upload Dataset

POST /api/files/upload

Create Spark Job

POST /api/jobs

Get Job Status

GET /api/jobs/{job_id}

Get Performance Metrics

GET /api/jobs/{job_id}/metrics


---

âš™ï¸ Configuration

Variable	Description	Default

STORAGE_BACKEND	local / s3	local
AWS_REGION	AWS region	us-east-1
DATABRICKS_HOST	Databricks workspace URL	â€”
DATABRICKS_TOKEN	Databricks token	â€”



---

ğŸ“ Academic Notes

Large datasets from UCI Machine Learning Repository are used

The project demonstrates parallel processing, distributed ML, and scalability

Designed for academic evaluation and cloud computing coursework
Conclusion

This project demonstrates how distributed Spark-based analytics and machine learning can be delivered as a cloud service with performance evaluation and scalability analysis.
